{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 一整頁文章一起跑\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./Workspace/data/dictionary'):\n",
    "    os.makedirs('./Workspace/data/dictionary')\n",
    "if not os.path.isfile('./Workspace/data/dictionary/autodict_v1.txt'):\n",
    "    with open('./Workspace/data/dictionary/autodict_v1.txt', 'w'):\n",
    "        pass\n",
    "    \n",
    "#移除標點符號\n",
    "def removePunctuation(source):\n",
    "    xx = u\"([^\\u4e00-\\u9fff]+)\"\n",
    "    s = re.sub(xx,' ',source)\n",
    "    return s\n",
    "\n",
    "#移除標籤，這邊沒用到\n",
    "def removeTag(soup,tag):\n",
    "    [x.extract() for x in soup.select(tag)]\n",
    "        \n",
    "#計算entropy，偷來改的，懶得改輸入型態，他是設計成list輸入，可一次輸出多Ngram的entropy，我只用單值\n",
    "def entropy(n_gram_lengths, text):\n",
    "    entropies = {}\n",
    "    #可逐一將list的n代入，不過我都一次只丟一個n進去\n",
    "    for n in n_gram_lengths:\n",
    "        #開始做n-gram\n",
    "        length_n_substrings = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        #計算字頻\n",
    "        length_n_substring_counts = Counter(length_n_substrings)\n",
    "        #斷出來的總數\n",
    "        normalizer = float(len(length_n_substrings))\n",
    "        #標準化\n",
    "        distribution_values = np.array(length_n_substring_counts.values()) / normalizer\n",
    "        #計算entropy\n",
    "        entropy = -np.sum(p*np.log2(p) for p in distribution_values)\n",
    "        #把結果丟進list\n",
    "        entropies[n] = entropy\n",
    "    #輸出    \n",
    "    return entropies\n",
    "\n",
    "\n",
    "#抓蘋果即時新聞\n",
    "def getAll(section, page):\n",
    "    domain = 'http://www.appledaily.com.tw'\n",
    "    res = requests.get('http://www.appledaily.com.tw/realtimenews/section/%s/%d' % (section, page))\n",
    "    soup = bs(res.text, 'html.parser')\n",
    "    urls = [domain+t['href'] for t in soup.select('.rtddt a') if 'appledaily' not in t['href']]\n",
    "    #用url來爬內容\n",
    "    return getContent(urls)\n",
    "\n",
    "#爬內容\n",
    "def getContent(urls):\n",
    "    news = ''\n",
    "    for u in urls:\n",
    "        res = requests.get(u)\n",
    "        soup = bs(res.text,'html.parser')\n",
    "        apple = soup.select('#summary')[0].text\n",
    "        #文章合併\n",
    "        news += removePunctuation(apple)\n",
    "    return news\n",
    "\n",
    "\n",
    "#頁數範圍\n",
    "for p in xrange(1,57):\n",
    "    #獲取一整頁的文章\n",
    "    news = getAll('new',p)\n",
    "    #斷句\n",
    "    sp = news.split()\n",
    "    #跑2-gram至4-gram\n",
    "    for n in xrange(2,5):\n",
    "        #存字頻的字典\n",
    "        substring_counts_dict = {}\n",
    "        #逐句做n-gram\n",
    "        for s in sp:\n",
    "            sentence_substrings = [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "            #計算字頻\n",
    "            for word in sentence_substrings:\n",
    "                if word not in substring_counts_dict:\n",
    "                    substring_counts_dict[word] = 1\n",
    "                else:\n",
    "                    substring_counts_dict[word] += 1\n",
    "        #字頻排序\n",
    "        word_freq = sorted(substring_counts_dict.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "        #計算完整entropy\n",
    "        org = entropy([n], news).values()[0]\n",
    "        #空字典，拿來存排除斷詞後的entropy\n",
    "        word_entropy = {}\n",
    "        #留下字頻大於5的斷詞\n",
    "        word_freq = [w for w in word_freq if w[1]>=5]\n",
    "        #分別將斷詞結果刪除，計算entropy\n",
    "        for w in word_freq:\n",
    "            word_entropy[w[0]] = entropy([n], news.replace(w[0],'')).values()[0]\n",
    "            #單純用字頻做限制的結果\n",
    "            #print w[0], w[1]\n",
    "        #用entropy排序\n",
    "        weighted_world = sorted(word_entropy.iteritems(),key=operator.itemgetter(1),reverse=False)\n",
    "        #逐一檢查篩選後之斷詞結果，留下entropy變化比例>=0.0001的結果\n",
    "        for keyword in weighted_world:\n",
    "            if (org-keyword[1])/org >= 0.0001:\n",
    "                with open('./Workspace/data/dictionary/autodict_v1.txt', 'a') as f:\n",
    "                    #輸出資訊變化比重\n",
    "                    #f.write('%s %s %s' % (keyword[0].encode('utf-8'), str((org-keyword[1])/org), '\\n'))\n",
    "                    f.write('%s%s' % (keyword[0].encode('utf-8'), '\\n'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
