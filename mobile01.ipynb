{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retain_English_Chinese_Arabic_numerals(StrIn):\n",
    "    Str_English_Chinese = u'([^a-zA-Zａ-ｚＡ-Ｚ\\u4E00-\\u9FCC]+)'\n",
    "    #Str_English_Chinese = u'([^a-z^A-Z^ａ-ｚ^Ａ-Ｚ^^0-9^０-９^\\u3105-\\u3129^\\u4E00-\\u9FCC]+)'\n",
    "    #\\u3105-\\u3129為所有注音符號 \n",
    "    #\\u4E00-\\u9FCC為所有中文\n",
    "    strClean = re.sub(Str_English_Chinese,' ',StrIn)\n",
    "    return strClean\n",
    "def deleteBadWords(StrIn):\n",
    "    Str_BadWords = u'([0-9]|[０-９]|[ㄅ-ㄩ]|延伸閱讀|連絡方式|電話預約|電話|營業時間|週一|週二|週三|週四|週五|週六|週日|周一|周二|周三|周四|周五|周六|\\\n",
    "                    |周日|假日|公休|平日|地址|粉絲團|星期|禮拜|時間限制|您或許對這些文章有興趣|造訪日期|全年無休|最後點餐|營業|AM|PM|上一篇|下一篇|\\\n",
    "                    |分享此文|您可能喜歡的文章|懶人包|臉書|Facebook|facebook|FB|fb|全世界便宜住宿看這兒|下載愛食記App隨時觀看|按個讚啦|喜歡我的分享嗎|\\\n",
    "                    |瘋台灣民宿網|官方網站|瀏覽人次|最新消息|餐廳名稱|消費時間|無圖文版|網誌|Postedonby|新鮮關注回聲|Christabelle的藝想世界部落格由製作|\\\n",
    "                    |也許對這些文章也有興趣|發表迴響|電子郵件|必要欄位標記|電子郵件|個人網站|輸入圖片顯示文字好證明你不是機器人|站內搜尋分類|最新動態|\\\n",
    "                    |並不會被公開|你的位址 |迴響名稱|用餐日期|留言|載入中|文章文章|粉絲頁|發表|每人平均價位|按個讚|推薦你閱讀|Instagram|instagram|\\\n",
    "                    |美食地圖|版權所有|網友回應|歡迎加入|標籤|著作權聲明|非經授權|不得轉載 )'\n",
    "    strClean = re.sub(Str_BadWords,'',StrIn)\n",
    "    return strClean\n",
    "def EnglishFullToHalf(StrIn):\n",
    "    return StrIn.replace(u'Ａ',u'A').replace(u'Ｂ',u'B').replace(u'Ｃ',u'C').replace(u'Ｄ',u'D')\\\n",
    "                .replace(u'Ｅ',u'E').replace(u'Ｆ',u'F').replace(u'Ｇ',u'G').replace(u'Ｈ',u'H')\\\n",
    "                .replace(u'Ｉ',u'I').replace(u'Ｊ',u'J').replace(u'Ｋ',u'K').replace(u'Ｌ',u'L')\\\n",
    "                .replace(u'Ｍ',u'M').replace(u'Ｎ',u'N').replace(u'Ｏ',u'O').replace(u'Ｐ',u'P')\\\n",
    "                .replace(u'Ｑ',u'Q').replace(u'Ｒ',u'R').replace(u'Ｓ',u'S').replace(u'Ｔ',u'T')\\\n",
    "                .replace(u'Ｕ',u'U').replace(u'Ｖ',u'V').replace(u'Ｗ',u'W').replace(u'Ｘ',u'X')\\\n",
    "                .replace(u'Ｙ',u'Y').replace(u'Ｚ',u'Z').replace(u'ａ',u'a').replace(u'ｂ',u'b')\\\n",
    "                .replace(u'ｃ',u'c').replace(u'ｄ',u'd').replace(u'ｅ',u'e').replace(u'ｆ',u'f')\\\n",
    "                .replace(u'ｇ',u'g').replace(u'ｈ',u'h').replace(u'ｉ',u'i').replace(u'ｊ',u'j')\\\n",
    "                .replace(u'ｋ',u'k').replace(u'ｌ',u'l').replace(u'ｍ',u'm').replace(u'ｎ',u'n')\\\n",
    "                .replace(u'ｏ',u'o').replace(u'ｐ',u'p').replace(u'ｑ',u'q').replace(u'ｒ',u'r')\\\n",
    "                .replace(u'ｓ',u's').replace(u'ｔ',u't').replace(u'ｕ',u'u').replace(u'ｖ',u'v')\\\n",
    "                .replace(u'ｗ',u'w').replace(u'ｘ',u'x').replace(u'ｙ',u'y').replace(u'ｚ',u'z')\n",
    "def removeTag(soup,tag):\n",
    "    [x.extract() for x in soup.select(tag)]\n",
    "\n",
    "brand=['HTC','Sony','Asus','Acer','Samsung','LG','Motorola','InFocus','GSmart','OPPO','TWM','OKWAP','HUAWEI']\n",
    "model=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "rs = requests.session()\n",
    "rs.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 4.{0}.{1};{2} {3}{4}-{5}{6}) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Version/{7}.0 Chrome/30.0.0.0 Mobile Safari/537.36\"\\\n",
    "        .format(random.randint(0,9),random.randint(0,9),brand[random.randint(0,len(brand)-1)],\\\n",
    "        model[random.randint(0,len(model)-1)],random.randint(1,99),model[random.randint(0,len(model)-1)],\\\n",
    "        random.randint(799,1599),random.randint(250,9999))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "def geturl(p, rs):\n",
    "    urls = []\n",
    "    domain = 'http://www.mobile01.com/'\n",
    "    #url = 'http://www.mobile01.com/waypointtopiclist.php?f=189&p={}'\n",
    "    url = 'http://www.mobile01.com/waypointforumtopic.php?p={}'\n",
    "    res = rs.get(url.format(p))\n",
    "    soup = bs(res.text,'html.parser')\n",
    "    page = int(re.search('共(.+)頁', soup.select('.numbers')[0].encode('utf-8')).group(1))\n",
    "    titles = soup.select('.subject-text')\n",
    "    for t in titles:\n",
    "        urls.append(domain+t.select('a')[0]['href'])\n",
    "    return urls\n",
    "\n",
    "def getFinalPage(url, rs):\n",
    "    res = rs.get(url)\n",
    "    soup = bs(res.text,'lxml')\n",
    "    page = int(re.search('共(.+)頁', soup.select('.numbers')[0].encode('utf-8')).group(1))\n",
    "    return page\n",
    "\n",
    "def getcontent(url, rs):\n",
    "    p = 1\n",
    "    review = ''\n",
    "    while(True): \n",
    "        res = rs.get(url+'&p=%d' % p)\n",
    "        soup = bs(res.text,'html.parser')\n",
    "        page = int(re.search('共(.+)頁', soup.select('.numbers')[0].encode('utf-8')).group(1))\n",
    "        removeTag(soup,'script')\n",
    "        removeTag(soup,'a')\n",
    "        removeTag(soup,'.rank')\n",
    "        removeTag(soup,'iframe')\n",
    "        removeTag(soup,'.fsb-social-bar')\n",
    "        removeTag(soup,'small')\n",
    "        removeTag(soup,'.comment-content.comment')\n",
    "        removeTag(soup,'.moreincategories.clearfix')\n",
    "        removeTag(soup,'.relativepost.clearfix')\n",
    "        removeTag(soup,'.auth')\n",
    "        removeTag(soup,'.store')\n",
    "        removeTag(soup,'.comments-area')\n",
    "        removeTag(soup,'.sharedaddy.sd-sharing-enabled')\n",
    "        removeTag(soup,'.vcard')\n",
    "        removeTag(soup,'#facebook')\n",
    "        removeTag(soup,'#sidebar')\n",
    "        removeTag(soup,'#jp-relatedposts')\n",
    "        removeTag(soup,'.hot-info')\n",
    "        removeTag(soup,'.agoda-link')\n",
    "        removeTag(soup,'#notice')\n",
    "        removeTag(soup,'.tag')\n",
    "        removeTag(soup,'blockquote')\n",
    "        for t in soup.select('.single-post-content'):\n",
    "            s =  \"\".join(\"\".join(t.text).split())\n",
    "            s = retain_English_Chinese_Arabic_numerals(s)\n",
    "            s = deleteBadWords(s)\n",
    "            s = EnglishFullToHalf(s).strip()+' '\n",
    "            review += s\n",
    "        if p < page:\n",
    "            p += 1\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            break\n",
    "    return review\n",
    "def getall(p):\n",
    "    st = ''\n",
    "    for u in geturl(p, rs):\n",
    "        try:\n",
    "            st += getcontent(u, rs)\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            print u\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# 一整頁文章一起跑\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./Workspace/data/dictionary'):\n",
    "    os.makedirs('./Workspace/data/dictionary')\n",
    "if not os.path.isfile('./Workspace/data/dictionary/autodict_m1.txt'):\n",
    "    with open('./Workspace/data/dictionary/autodict_m1.txt', 'w'):\n",
    "        pass\n",
    "\n",
    "#計算entropy，偷來改的，懶得改輸入型態，他是設計成list輸入，可一次輸出多Ngram的entropy，我只用單值\n",
    "def entropy(n_gram_lengths, text):\n",
    "    entropies = {}\n",
    "    #可逐一將list的n代入，不過我都一次只丟一個n進去\n",
    "    for n in n_gram_lengths:\n",
    "        #開始做n-gram\n",
    "        length_n_substrings = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        #計算字頻\n",
    "        length_n_substring_counts = Counter(length_n_substrings)\n",
    "        #斷出來的總數\n",
    "        normalizer = float(len(length_n_substrings))\n",
    "        #標準化\n",
    "        distribution_values = np.array(length_n_substring_counts.values()) / normalizer\n",
    "        #計算entropy\n",
    "        entropy = -np.sum(p*np.log2(p) for p in distribution_values)\n",
    "        #把結果丟進list\n",
    "        entropies[n] = entropy\n",
    "    #輸出    \n",
    "    return entropies    \n",
    "\n",
    "#頁數範圍\n",
    "fp = getFinalPage('http://www.mobile01.com/waypointforumtopic.php', rs)\n",
    "for p in xrange(1,fp+1):\n",
    "    print p\n",
    "    #獲取一整頁的文章\n",
    "    news = getall(p)\n",
    "    #斷句\n",
    "    sp = news.split()\n",
    "    #跑2-gram至4-gram\n",
    "    for n in xrange(2,5):\n",
    "        #存字頻的字典\n",
    "        substring_counts_dict = {}\n",
    "        #逐句做n-gram\n",
    "        for s in sp:\n",
    "            sentence_substrings = [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "            #計算字頻\n",
    "            for word in sentence_substrings:\n",
    "                if word not in substring_counts_dict:\n",
    "                    substring_counts_dict[word] = 1\n",
    "                else:\n",
    "                    substring_counts_dict[word] += 1\n",
    "        #字頻排序\n",
    "        word_freq = sorted(substring_counts_dict.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "        #計算完整entropy\n",
    "        org = entropy([n], news).values()[0]\n",
    "        #空字典，拿來存排除斷詞後的entropy\n",
    "        word_entropy = {}\n",
    "        #留下字頻大於5的斷詞\n",
    "        word_freq = [w for w in word_freq if w[1]>=5]\n",
    "        #分別將斷詞結果刪除，計算entropy\n",
    "        for w in word_freq:\n",
    "            word_entropy[w[0]] = entropy([n], news.replace(w[0],'')).values()[0]\n",
    "            #單純用字頻做限制的結果\n",
    "            #print w[0], w[1]\n",
    "        #用entropy排序\n",
    "        weighted_world = sorted(word_entropy.iteritems(),key=operator.itemgetter(1),reverse=False)\n",
    "        #逐一檢查篩選後之斷詞結果，留下entropy變化比例>=0.0001的結果\n",
    "        for keyword in weighted_world:\n",
    "            if (org-keyword[1])/org >= 0.0001:\n",
    "                with open('./Workspace/data/dictionary/autodict_m1.txt', 'a') as f:\n",
    "                    #輸出資訊變化比重\n",
    "                    #f.write('%s %s %s' % (keyword[0].encode('utf-8'), str((org-keyword[1])/org), '\\n'))\n",
    "                    f.write('%s%s' % (keyword[0].encode('utf-8'), '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6492\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import re\n",
    "temp = set()\n",
    "for fname in os.listdir('./Workspace/data/dictionary'):\n",
    "    if 'txt' in fname:\n",
    "        with codecs.open('./Workspace/data/dictionary/%s' %fname, 'r', 'utf-8') as f:\n",
    "            for t in f.readlines():\n",
    "                if not re.search('\\w', t) and u'的' not in t:\n",
    "                    temp.add(t.strip('\\n'))\n",
    "with open('./Workspace/data/dictionary/all/dictionary.txt', 'w') as f:\n",
    "    for x in list(temp):\n",
    "        f.write(x.encode('utf-8')+'\\n')\n",
    "print len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
