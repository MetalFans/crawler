{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def retain_English_Chinese_Arabic_numerals(StrIn):\n",
    "    Str_English_Chinese = u'([^0-9a-zA-Zａ-ｚＡ-Ｚ\\u4E00-\\u9FCC]+)'\n",
    "    #Str_English_Chinese = u'([^a-z^A-Z^ａ-ｚ^Ａ-Ｚ^^0-9^０-９^\\u3105-\\u3129^\\u4E00-\\u9FCC]+)'\n",
    "    #\\u3105-\\u3129為所有注音符號 \n",
    "    #\\u4E00-\\u9FCC為所有中文\n",
    "    strClean = re.sub(Str_English_Chinese,' ',StrIn)\n",
    "    return strClean\n",
    "def deleteBadWords(StrIn):\n",
    "    Str_BadWords = u'([a-zA-Z]|[0-9]|[０-９]|[ㄅ-ㄩ]|延伸閱讀|連絡方式|電話預約|電話|營業時間|週一|週二|週三|週四|週五|週六|週日|周一|周二|周三|周四|周五|周六|\\\n",
    "                    |周日|假日|公休|平日|地址|粉絲團|星期|禮拜|時間限制|您或許對這些文章有興趣|造訪日期|全年無休|最後點餐|營業|AM|PM|上一篇|下一篇|\\\n",
    "                    |分享此文|您可能喜歡的文章|懶人包|臉書|Facebook|facebook|FB|fb|全世界便宜住宿看這兒|下載愛食記App隨時觀看|按個讚啦|喜歡我的分享嗎|\\\n",
    "                    |瘋台灣民宿網|官方網站|瀏覽人次|最新消息|餐廳名稱|消費時間|無圖文版|網誌|Postedonby|新鮮關注回聲|Christabelle的藝想世界部落格由製作|\\\n",
    "                    |也許對這些文章也有興趣|發表迴響|電子郵件|必要欄位標記|電子郵件|個人網站|輸入圖片顯示文字好證明你不是機器人|站內搜尋分類|最新動態|\\\n",
    "                    |並不會被公開|你的位址 |迴響名稱|用餐日期|留言|載入中|文章文章|粉絲頁|發表|每人平均價位|按個讚|推薦你閱讀|Instagram|instagram|\\\n",
    "                    |美食地圖|版權所有|網友回應|歡迎加入|標籤|著作權聲明|非經授權|不得轉載 )'\n",
    "    strClean = re.sub(Str_BadWords,'',StrIn)\n",
    "    return strClean\n",
    "def EnglishFullToHalf(StrIn):\n",
    "    return StrIn.replace(u'Ａ',u'A').replace(u'Ｂ',u'B').replace(u'Ｃ',u'C').replace(u'Ｄ',u'D')\\\n",
    "                .replace(u'Ｅ',u'E').replace(u'Ｆ',u'F').replace(u'Ｇ',u'G').replace(u'Ｈ',u'H')\\\n",
    "                .replace(u'Ｉ',u'I').replace(u'Ｊ',u'J').replace(u'Ｋ',u'K').replace(u'Ｌ',u'L')\\\n",
    "                .replace(u'Ｍ',u'M').replace(u'Ｎ',u'N').replace(u'Ｏ',u'O').replace(u'Ｐ',u'P')\\\n",
    "                .replace(u'Ｑ',u'Q').replace(u'Ｒ',u'R').replace(u'Ｓ',u'S').replace(u'Ｔ',u'T')\\\n",
    "                .replace(u'Ｕ',u'U').replace(u'Ｖ',u'V').replace(u'Ｗ',u'W').replace(u'Ｘ',u'X')\\\n",
    "                .replace(u'Ｙ',u'Y').replace(u'Ｚ',u'Z').replace(u'ａ',u'a').replace(u'ｂ',u'b')\\\n",
    "                .replace(u'ｃ',u'c').replace(u'ｄ',u'd').replace(u'ｅ',u'e').replace(u'ｆ',u'f')\\\n",
    "                .replace(u'ｇ',u'g').replace(u'ｈ',u'h').replace(u'ｉ',u'i').replace(u'ｊ',u'j')\\\n",
    "                .replace(u'ｋ',u'k').replace(u'ｌ',u'l').replace(u'ｍ',u'm').replace(u'ｎ',u'n')\\\n",
    "                .replace(u'ｏ',u'o').replace(u'ｐ',u'p').replace(u'ｑ',u'q').replace(u'ｒ',u'r')\\\n",
    "                .replace(u'ｓ',u's').replace(u'ｔ',u't').replace(u'ｕ',u'u').replace(u'ｖ',u'v')\\\n",
    "                .replace(u'ｗ',u'w').replace(u'ｘ',u'x').replace(u'ｙ',u'y').replace(u'ｚ',u'z')\n",
    "def removeTag(soup,tag):\n",
    "    [x.extract() for x in soup.select(tag)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import Counter\n",
    "import os\n",
    "brand=['HTC','Sony','Asus','Acer','Samsung','LG','Motorola','InFocus','GSmart','OPPO','TWM','OKWAP','HUAWEI']\n",
    "model=['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "rs = requests.session()\n",
    "rs.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 4.{0}.{1};{2} {3}{4}-{5}{6}) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Version/{7}.0 Chrome/30.0.0.0 Mobile Safari/537.36\"\\\n",
    "        .format(random.randint(0,9),random.randint(0,9),brand[random.randint(0,len(brand)-1)],\\\n",
    "        model[random.randint(0,len(model)-1)],random.randint(1,99),model[random.randint(0,len(model)-1)],\\\n",
    "        random.randint(799,1599),random.randint(250,9999))\n",
    "    })\n",
    "def geturl(rs, p):\n",
    "    domain = 'https://tw.news.yahoo.com'\n",
    "    res = rs.get('https://tw.news.yahoo.com/travel/archive/%d.html' %p)\n",
    "    soup = bs(res.text, 'html.parser')\n",
    "    temp = soup.select('.txt')\n",
    "    href = [domain+x.select('a')[0]['href'] for x in temp if 'video' not in x.select('a')[0]['href'] and '#' not in x.select('a')[0]['href']]\n",
    "    return href\n",
    "\n",
    "def getcontent(rs, url):\n",
    "    res = rs.get(url)\n",
    "    #res.encoding = res.apparent_encoding()\n",
    "    soup = bs(res.text, 'html.parser')\n",
    "    removeTag(soup,'script')\n",
    "    removeTag(soup,'a')\n",
    "    removeTag(soup,'.rank')\n",
    "    removeTag(soup,'iframe')\n",
    "    removeTag(soup,'.fsb-social-bar')\n",
    "    removeTag(soup,'small')\n",
    "    removeTag(soup,'.comment-content.comment')\n",
    "    removeTag(soup,'.moreincategories.clearfix')\n",
    "    removeTag(soup,'.relativepost.clearfix')\n",
    "    removeTag(soup,'.auth')\n",
    "    removeTag(soup,'.store')\n",
    "    removeTag(soup,'.comments-area')\n",
    "    removeTag(soup,'.sharedaddy.sd-sharing-enabled')\n",
    "    removeTag(soup,'.vcard')\n",
    "    removeTag(soup,'#facebook')\n",
    "    removeTag(soup,'#sidebar')\n",
    "    removeTag(soup,'#jp-relatedposts')\n",
    "    removeTag(soup,'.hot-info')\n",
    "    removeTag(soup,'.agoda-link')\n",
    "    removeTag(soup,'#notice')\n",
    "    removeTag(soup,'.tag')\n",
    "    s = ''.join(t.text for t in soup.select('.bd'))\n",
    "    s =  \"\".join(\"\".join(s).split())\n",
    "    s = retain_English_Chinese_Arabic_numerals(s)\n",
    "    s = deleteBadWords(s)\n",
    "    s = EnglishFullToHalf(s).strip()+' '\n",
    "    return s\n",
    "\n",
    "def getall(rs, p):\n",
    "    st = ''\n",
    "    for u in geturl(rs, p):\n",
    "        try:\n",
    "            st += getcontent(rs, u)\n",
    "        except:\n",
    "            print u\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://tw.news.yahoo.com/蘇花九宮里土石流-單線管制通行-063606809.html\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./Workspace/data/dictionary'):\n",
    "    os.makedirs('./Workspace/data/dictionary')\n",
    "if not os.path.isfile('./Workspace/data/dictionary/autodict_y1.txt'):\n",
    "    with open('./Workspace/data/dictionary/autodict_y1.txt', 'w'):\n",
    "        pass\n",
    "\n",
    "#計算entropy，偷來改的，懶得改輸入型態，他是設計成list輸入，可一次輸出多Ngram的entropy，我只用單值\n",
    "def entropy(n_gram_lengths, text):\n",
    "    entropies = {}\n",
    "    #可逐一將list的n代入，不過我都一次只丟一個n進去\n",
    "    for n in n_gram_lengths:\n",
    "        #開始做n-gram\n",
    "        length_n_substrings = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "        #計算字頻\n",
    "        length_n_substring_counts = Counter(length_n_substrings)\n",
    "        #斷出來的總數\n",
    "        normalizer = float(len(length_n_substrings))\n",
    "        #標準化\n",
    "        distribution_values = np.array(length_n_substring_counts.values()) / normalizer\n",
    "        #計算entropy\n",
    "        entropy = -np.sum(p*np.log2(p) for p in distribution_values)\n",
    "        #把結果丟進list\n",
    "        entropies[n] = entropy\n",
    "    #輸出    \n",
    "    return entropies    \n",
    "    \n",
    "#頁數範圍\n",
    "res = requests.get('https://tw.news.yahoo.com/travel/archive/')\n",
    "soup = bs(res.text, 'html.parser')\n",
    "finalpage = int(soup.select('a.last')[0]['href'].split('/')[-1].split('.html')[0])\n",
    "for p in xrange(1,finalpage):\n",
    "    #獲取一整頁的文章\n",
    "    try:\n",
    "        news = getall(rs, p)\n",
    "    except:\n",
    "        print p\n",
    "    #斷句\n",
    "    sp = news.split()\n",
    "    #跑2-gram至4-gram\n",
    "    for n in xrange(2,5):\n",
    "        #存字頻的字典\n",
    "        substring_counts_dict = {}\n",
    "        #逐句做n-gram\n",
    "        for s in sp:\n",
    "            sentence_substrings = [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "            #計算字頻\n",
    "            for word in sentence_substrings:\n",
    "                if word not in substring_counts_dict:\n",
    "                    substring_counts_dict[word] = 1\n",
    "                else:\n",
    "                    substring_counts_dict[word] += 1\n",
    "        #字頻排序\n",
    "        word_freq = sorted(substring_counts_dict.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "        #計算完整entropy\n",
    "        org = entropy([n], news).values()[0]\n",
    "        #空字典，拿來存排除斷詞後的entropy\n",
    "        word_entropy = {}\n",
    "        #留下字頻大於5的斷詞\n",
    "        word_freq = [w for w in word_freq if w[1]>=5]\n",
    "        #分別將斷詞結果刪除，計算entropy\n",
    "        for w in word_freq:\n",
    "            word_entropy[w[0]] = entropy([n], news.replace(w[0],'')).values()[0]\n",
    "            #單純用字頻做限制的結果\n",
    "            #print w[0], w[1]\n",
    "        #用entropy排序\n",
    "        weighted_world = sorted(word_entropy.iteritems(),key=operator.itemgetter(1),reverse=False)\n",
    "        #逐一檢查篩選後之斷詞結果，留下entropy變化比例>=0.0001的結果\n",
    "        for keyword in weighted_world:\n",
    "            if (org-keyword[1])/org >= 0.0001:\n",
    "                with open('./Workspace/data/dictionary/autodict_y1.txt', 'a') as f:\n",
    "                    #輸出資訊變化比重\n",
    "                    #f.write('%s %s %s' % (keyword[0].encode('utf-8'), str((org-keyword[1])/org), '\\n'))\n",
    "                    f.write('%s%s' % (keyword[0].encode('utf-8'), '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
